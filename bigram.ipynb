{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7554288,"sourceType":"datasetVersion","datasetId":4399783}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nblock_size = 8\nbatch_size = 4\nmax_iters = 1000\neval_iters = 250\nlearning_rate = 3e-4","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-11T10:36:12.131633Z","iopub.execute_input":"2024-02-11T10:36:12.132265Z","iopub.status.idle":"2024-02-11T10:36:14.324663Z","shell.execute_reply.started":"2024-02-11T10:36:12.132215Z","shell.execute_reply":"2024-02-11T10:36:14.323356Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/wizard-of-oz/wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\nchars = sorted(set(text))\nprint(chars)\nvocab_size = len(chars)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:36:14.326761Z","iopub.execute_input":"2024-02-11T10:36:14.327388Z","iopub.status.idle":"2024-02-11T10:36:14.370062Z","shell.execute_reply.started":"2024-02-11T10:36:14.327342Z","shell.execute_reply":"2024-02-11T10:36:14.368832Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"['\\t', '\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n","output_type":"stream"}]},{"cell_type":"code","source":"string_to_int = {ch : i for i, ch in enumerate(chars)}\nint_to_string = {i : ch for i, ch in enumerate(chars)}\nencode = lambda s: [string_to_int[c] for c in s]\ndecode = lambda l: ''.join([int_to_string[i] for i in l])\n\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data[:100])","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:36:14.373801Z","iopub.execute_input":"2024-02-11T10:36:14.375382Z","iopub.status.idle":"2024-02-11T10:36:14.534581Z","shell.execute_reply.started":"2024-02-11T10:36:14.375338Z","shell.execute_reply":"2024-02-11T10:36:14.533302Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"tensor([81,  1,  1,  0,  0,  2,  2, 29, 40, 43, 40, 45, 33, 50,  2, 26, 39, 29,\n         2, 45, 33, 30,  2, 48, 34, 51, 26, 43, 29,  2, 34, 39,  2, 40, 51,  1,\n         1,  2,  2, 27, 50,  1,  1,  2,  2, 37, 12,  2, 31, 43, 26, 39, 36,  2,\n        27, 26, 46, 38,  1,  1,  2,  2, 26, 46, 45, 33, 40, 43,  2, 40, 31,  2,\n        45, 33, 30,  2, 48, 34, 51, 26, 43, 29,  2, 40, 31,  2, 40, 51, 10,  2,\n        45, 33, 30,  2, 37, 26, 39, 29,  2, 40])\n","output_type":"stream"}]},{"cell_type":"code","source":"n = int(0.8*len(data))\ntrain_data = data[:n]\ntest_data = data[n:]\n","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:36:14.537861Z","iopub.execute_input":"2024-02-11T10:36:14.539265Z","iopub.status.idle":"2024-02-11T10:36:14.545821Z","shell.execute_reply.started":"2024-02-11T10:36:14.539218Z","shell.execute_reply":"2024-02-11T10:36:14.544410Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"x = train_data[:block_size]\ny = train_data[1:block_size+1]\n\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print('When input is ', context, 'Output is ', target)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:36:14.548498Z","iopub.execute_input":"2024-02-11T10:36:14.549656Z","iopub.status.idle":"2024-02-11T10:36:14.563976Z","shell.execute_reply.started":"2024-02-11T10:36:14.549610Z","shell.execute_reply":"2024-02-11T10:36:14.562217Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"When input is  tensor([81]) Output is  tensor(1)\nWhen input is  tensor([81,  1]) Output is  tensor(1)\nWhen input is  tensor([81,  1,  1]) Output is  tensor(0)\nWhen input is  tensor([81,  1,  1,  0]) Output is  tensor(0)\nWhen input is  tensor([81,  1,  1,  0,  0]) Output is  tensor(2)\nWhen input is  tensor([81,  1,  1,  0,  0,  2]) Output is  tensor(2)\nWhen input is  tensor([81,  1,  1,  0,  0,  2,  2]) Output is  tensor(29)\nWhen input is  tensor([81,  1,  1,  0,  0,  2,  2, 29]) Output is  tensor(40)\n","output_type":"stream"}]},{"cell_type":"code","source":"#Explanation of how batches work\nn = int(0.8*len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n\ndef get_batch(split):\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n#     print(ix)\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n#     x, y = x.to(device), y.to(device)\n    return x,y\n\nx,y = get_batch('train')\nprint('inputs:')\nprint(x)\nprint('targets:')\nprint(y)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:36:14.566288Z","iopub.execute_input":"2024-02-11T10:36:14.566955Z","iopub.status.idle":"2024-02-11T10:36:14.615102Z","shell.execute_reply.started":"2024-02-11T10:36:14.566915Z","shell.execute_reply":"2024-02-11T10:36:14.613974Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"inputs:\ntensor([[62, 69, 66, 59, 25,  4,  2, 55],\n        [79, 69, 75,  2, 74, 77, 69, 10],\n        [74,  2, 63, 74,  2, 69, 75, 74],\n        [58,  2, 62, 63, 67, 12,  2, 34]])\ntargets:\ntensor([[69, 66, 59, 25,  4,  2, 55, 73],\n        [69, 75,  2, 74, 77, 69, 10,  2],\n        [ 2, 63, 74,  2, 69, 75, 74, 12],\n        [ 2, 62, 63, 67, 12,  2, 34, 74]])\n","output_type":"stream"}]},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train','test']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X,Y = get_batch(split)\n            logits, loss = model(X,Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:36:14.616644Z","iopub.execute_input":"2024-02-11T10:36:14.618327Z","iopub.status.idle":"2024-02-11T10:36:14.627231Z","shell.execute_reply.started":"2024-02-11T10:36:14.618283Z","shell.execute_reply":"2024-02-11T10:36:14.626031Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#Bigram Language Model\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n    \n    def forward(self, index, targets = None):\n        #logits is like softmax for each index\n        logits = self.token_embedding_table(index)\n        if targets is None:\n            loss = None\n        else:\n            #Unpacks dimensions for batch size (B), sequence length (T), and vocab size (C).\n            B, T, C = logits.shape\n            #Reshapes logits to combine all words in the batch for efficient loss calculation.\n            logits = logits.view(B*T, C)\n            #Reshapes targets to match the new logits shape.\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        return logits, loss\n    \n    def generate(self, index, max_new_tokens):\n        #index is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self.forward(index)\n            #Focous only on the last time step\n            logits = logits[:, -1, :] # becomes (B,C)\n            #Apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) #(B,C)\n            # sample from the distribution\n            index_next = torch.multinomial(probs, num_samples=1) #(B,1)\n            #append sampled index to the running sequence\n            index = torch.cat((index, index_next), dim=1) #(B,T+1)\n        return index\n\nmodel = BigramLanguageModel(vocab_size)\n# m = model.to(device)\n\n# context = torch.zeros((1,1), dtype=torch.long, device = device)\ncontext = torch.zeros((1,1), dtype=torch.long)\ngenerated_chars = decode(model.generate(context, max_new_tokens=500)[0].tolist())\nprint(generated_chars)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:36:14.630187Z","iopub.execute_input":"2024-02-11T10:36:14.630829Z","iopub.status.idle":"2024-02-11T10:36:14.805105Z","shell.execute_reply.started":"2024-02-11T10:36:14.630787Z","shell.execute_reply":"2024-02-11T10:36:14.804003Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"\t h)Pbl,jm853f)S﻿j!U[!Tl2l9E I-E]n8htvOmVIk[59KlM.Ef)qSXRwaiz3u:z3s;!\nRGY[dx7lXN_l]&YEsXC1\tNqVI:4X;4\nc&om[OI4S\n&Btr\n1:j0Y!nc[tWc!!SU&Ve8ts_.4\ncB[O75v\"P;4CfYci9PjyZ9E(lE&P6 RwyJX﻿]n\tb]3T9e(A.Xy\tN&JHM;U[wKdxL88,i0pZ;WKlEu﻿PZwUO&V7M\"X:RZB﻿;Om8RGXIyXynWr4YHH!Hf)nYjJqqu﻿iIa03*Xta1,TJw(\t2kqN;SL03v.]1CrZyg2sn.*f)6.DU_VOa1]ox6JG?bPJXphBdoc&Q_9E( 4Y6r1o-Rw[31ovuAar6u&*lMqbu&lwZ'r_9X,ka]:(]'8\"_YDPWv'S&*do*3x6 I3TRwAiJ])1\taF _Y2t;\nLN2UsqK&2W6oK5L7uwuj3:i5Y]'P4cU'3fff8LzYp4geac,6hTcgFRVsMJ)z\n12Ut6w\n2?Pb\nW\"p\t\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create a Pytorch optimiser\noptimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\nfor iter in range(max_iters):\n    if iter % eval_iters == 0:\n        losses = estimate_loss()\n        print(f'step: {iter}, loss: {losses} ')\n    #sample a batch of data\n    xb, yb = get_batch('train')\n    #evaluate the loss\n    logits, loss = model.forward(xb, yb)\n    optimizer.zero_grad(set_to_none = True)\n    loss.backward()\n    optimizer.step()\nprint(loss)\nprint(loss.item())","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:36:14.810203Z","iopub.execute_input":"2024-02-11T10:36:14.813798Z","iopub.status.idle":"2024-02-11T10:36:19.778522Z","shell.execute_reply.started":"2024-02-11T10:36:14.813745Z","shell.execute_reply":"2024-02-11T10:36:19.777650Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"step: 0, loss: {'train': tensor(4.8784), 'test': tensor(4.8956)} \nstep: 250, loss: {'train': tensor(4.8136), 'test': tensor(4.8439)} \nstep: 500, loss: {'train': tensor(4.7404), 'test': tensor(4.7620)} \nstep: 750, loss: {'train': tensor(4.7139), 'test': tensor(4.7137)} \ntensor(4.5332, grad_fn=<NllLossBackward0>)\n4.533194541931152\n","output_type":"stream"}]},{"cell_type":"code","source":"context = torch.zeros((1,1), dtype=torch.long)\ngenerated_chars = decode(model.generate(context, max_new_tokens=500)[0].tolist())\nprint(generated_chars)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:36:19.781284Z","iopub.execute_input":"2024-02-11T10:36:19.781953Z","iopub.status.idle":"2024-02-11T10:36:19.862116Z","shell.execute_reply.started":"2024-02-11T10:36:19.781921Z","shell.execute_reply":"2024-02-11T10:36:19.860545Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\t9* _:dNe:(G2CC1b17RO]nuLVFw CM-y'l?UZ_cku:\"cuL\n_1,OM0iKCCMBeI3?h\"oVx7M,RwyYIVezYYc'-nCysmbGN6gUWX\tT,6m﻿cSoeEld\nD.GElK(a4bb.mRwXHinhOI\te&aeh!pz_[ps.PqPE]N﻿BEXQEdBzZG*y&v,h-Z4[S(MojSJ]nX2f-,jWfPAFGy)gGck\"0 nC!W*7fPvf),hMp_aXSJ!)3a[p;EgYcwf)GV;EzY:Vna7rGnk8]6f)Ki)dw'9&E(&V\t1mfdXW6YPRmqHilEvpkL\n﻿4hMVZtvbrvwZffFgfx6)Jlw'vmF;4KItKyQ_?VpQonv'B]TK(h_xu2J?8cSiy. ETU VphXuLCApqQag5n5Q0&lGcu&td?Apk!mhnW6ruw((*x rx4KierWK7_v,?1x7 eE(3aEv\"yO4[2fy\tQHJu﻿ZX-Umnff k:fs[!U﻿(&og[:ALa]\t5g2L\n'8RCK6hAlE:yf)nqilBo\n&OW\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}